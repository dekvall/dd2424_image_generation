\documentclass[../main.tex]{subfiles}
\begin{comment}
\addbibresource{../bib/bibliography.bib} 
%This is to get the autocomplete to work
\end{comment}
\begin{document}

%\subsection{Method choosing}
There have been are many attempts on Text to Image generation before, For example, Variational autoencoder(VAE), Generative Adversarial Network(GAN) and Seq2seq. However, the image produced by VAE is of low resolution that is too vague. For Seq2seq, although it can produce image with more details, it can only be used in generating abstract scenes from textual descriptions, instead of real image. Thus, we decide to using GAN to generate image from text.
\par
General adversarial networks are a fairly new approach to image generation. One such approach is the Deep-Convolutional Adversarial Network(DC-GAN)\cite{radford2015dcgan}. The basis of a GAN is to train two components, one generator and one discriminator. Basically the Discriminator is optimized to notice whether or not an image is created by the Generator and the Generator is optimized to create as realistic images as possible for the generator. In essence they play the game $V(G,D)$ where
\begin{equation}
    \min_G \max_D V(G,D) = \mathbb{E}_{x\sim p_{data}(x)}[log(D(x))] + \mathbb{E}_{z\sim p_{z}(z)}[1 - log(D(G(z)))]
\end{equation}
Which has a global optimum where $p_g = p_{data}$ and with sufficient data converges to that optimum \cite{goodfellow2014generative}. In the beginning of training though, samples from $G$ will be poor and has a high chance of being rejected by $D$ so it has been found out that in practice one should maximize $log(D(G(z)))$ instead of minimizing $log(1 - D(G(z)))$\cite{reed2016generative}.
\par
DC-GAN does use any information about the scene when creating images, since that is something we are intersted in, we look at an approach called ConditionalGAN  \cite{mirza2014conditionalgan} where we condition the Generator, $G$ and Discriminator $D$ with a conditioning value $c$ along with the latent vector $z$. This conditioning variable is associated with the text which describes the image. Furthermore, in \cite{zhang2017stackgan} the authors report using a technique called conditioning augmentation where instead of taking the text representation directly they sample the condition variables from a normal distribution around the true $c$.

The most common approach for speech recognition is Recurrent neural network, which have been successfully implemented in \cite{hannun2014deep} and \cite{sak2014long}. Convolution neural networks have also been used for speech recognition in \cite{speechcommandsv2}. The model is developed by Tensorflow and is specifically developed for their Speech Command dataset.

\end{document}
