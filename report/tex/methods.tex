%!TEX root=../main.tex
\documentclass[../main.tex]{subfiles}
\begin{comment}
\addbibresource{../bib/bibliography.bib}
\end{comment}
\begin{document}
\subsection{Text encoding}
The text embedding, $\phi(t)$, of the text description is generated by a pre-trained encoder \cite{reed2016learning}. We used a Word2vec \cite{mikolov2013word2vec} model trained on 3.3 billion Google news articles. In \cite{reed2016learning} the word2vec output was averaged so the sentence structure was lost, so we want to try another approach. In \cite{de2016representation} the authors argue that the concatenation of the $min$ and $max$ vector yields an adequate feature space representation of short texts.
\subsection{Image Generation}
The image generation is done with a Conditional GAN \cite{mirza2014conditionalgan} with a modification on the condition varible. In \cite{zhang2017stackgan} condition augmentation is used in which instead of just using the fixed text embedding $c = \varphi(t)$, they sample the latent variables $\hat{c}$ from a normal distribution. 
\begin{equation}
    \mathcal{N}(\mu (c),\Sigma (c))
\end{equation}
Where $\Sigma (c)$ is the diagonal covariance matrix. This sampling mitigates discontinuity in the latent data manifold and therefore makes (already unstable) training process more manageable. 


\end{document}
